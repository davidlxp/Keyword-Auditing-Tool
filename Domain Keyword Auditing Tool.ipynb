{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os.path\n",
    "import pickle\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "from string import ascii_uppercase\n",
    "from string import digits\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from threading import Thread #Support Multi-Threading Crawling\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import xlsxwriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the used Taxonomy ###\n",
    "\n",
    "taxonomy_name = 'github_brand domain_12022020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_loc = 'C:\\Domain Keyword Auditing\\{}.xlsx'.format(taxonomy_name)\n",
    "original_df = pd.read_excel(taxonomy_loc,sheet_name='input')\n",
    "original_df = original_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edit column name based on the Taxonomy type to fit the column name script use ###\n",
    "\n",
    "if 'generic' in taxonomy_name:\n",
    "    original_df = original_df.rename(columns={\"product\":\"brand\"})\n",
    "    original_df['category'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = copy.deepcopy(original_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 1. Preparing the Domain keyword data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for dedup a list and but keep its original sequence ###\n",
    "\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    # Using set to keep track of the unique vlaue \n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning the brand. Also clean the title, url from the search result website (remove all non-char)\n",
    "# eg. 'Rue89 - Home | Facebook https://www.facebook.com/Rue89' ---> ' rue89 home facebook https en gb facebook com rue89 '\n",
    "\n",
    "def source_cleaner(title):\n",
    "    pattern = re.compile('[^a-zA-Z0-9\\s]')  #An Regex pattern stands for all the upper and lower case letter and numbers, ^ means not\n",
    "    char_title_lst = pattern.sub(' ', title).split(' ')  # Substitute all elements which not number or char using ' '\n",
    "\n",
    "    char_title_lst = [x for x in char_title_lst if x != '']  # Getting all the not ' ' element from the list  \n",
    "    char_title = ' '.join(char_title_lst).lower()\n",
    "    \n",
    "    return(char_title)   #return cleaned title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: turning the brand into a list of brands (prepare for matching to crawled website text)\n",
    "\n",
    "def to_brand_list(brand_name):\n",
    "    \n",
    "    ### Function for trim the text ###\n",
    "    def trim(text):\n",
    "        new_text = ' '.join([t for t in text.split(\" \") if t != ''])\n",
    "        return(new_text)\n",
    "    \n",
    "    ### work on brand names ###\n",
    "    brand_name_lst = brand_name.split(',')  # Just in case if multiple brand name is given like \"verizon\" and \"verizon wireless\"\n",
    "    brand_name_lst = [trim(b) for b in brand_name_lst]\n",
    "    \n",
    "    brand_lst = []\n",
    "    \n",
    "    for brand in brand_name_lst:\n",
    "        \n",
    "        # 1. Originl form\n",
    "        # If you want to find a word within sentence, there is space around the wor. (eg. 'spotify' -> '(space) spotify (space)')\n",
    "        brand_lst.append(' {} '.format(brand))   \n",
    "        \n",
    "        # 2. Space removed form\n",
    "        if \" \" in brand:\n",
    "            brand_lst.append(' {} '.format(brand.replace(' ','')))  \n",
    "        \n",
    "        # 3. Special Char removed form\n",
    "        cleaned_brand = source_cleaner(brand)\n",
    "        if cleaned_brand != brand:\n",
    "            brand_lst.append(' {} '.format(cleaned_brand))\n",
    "        \n",
    "        # 4. Special Char & space removed form\n",
    "        if \" \" in cleaned_brand:\n",
    "            brand_lst.append(' {} '.format(cleaned_brand.replace(' ','')))  \n",
    "            \n",
    "    brand_lst = unique(brand_lst)\n",
    "    \n",
    "    return(brand_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: getting \"cleaned domain\" (without tail), lowered brand-term, \"brand_list\" for matching ###\n",
    "\n",
    "def to_new_df(df):\n",
    "    \n",
    "    # lower case the dataframe column name\n",
    "    df.columns = map(str.lower,df.columns)\n",
    "   \n",
    "    # Getting domain with or without tail removed (Based on need)\n",
    "    clean_domain = []\n",
    "    for i in range(len(df)):\n",
    "        domain = df['domain'][i]\n",
    "        clean_str = domain.lower()   # 1. New method: only change domain to lower case, not remove tail when search\n",
    "        clean_domain.append(clean_str)\n",
    "    df['clean_domain'] = clean_domain\n",
    "\n",
    "    # Clean the brand-term (remove extra space caused by manually input)    \n",
    "    # Because 'space' will be used as a important judgement in function to_brand_list, we couldn't allow misinputed 'space'\n",
    "    # eg. \" hotel.com   \" -> \"hotel.com\"\n",
    "    \n",
    "    # Some brand will give multiple name like verizon, \"verizon\" and \"verizon wireless\"\n",
    "    df['brand'] = df['brand'].apply(lambda x : ','.join([\" \".join([each for each in b.split(' ') if each != '']) for b in x.split(',')]))\n",
    "    df['brand_low'] = df['brand'].str.lower()  # Turn a whole column into lower    \n",
    "    \n",
    "    # Getting brand_list for matching\n",
    "    df['brand_list'] = df['brand_low'].apply(to_brand_list)\n",
    "    \n",
    "    # Some brand will give multiple name like verizon, \"verizon\" and \"verizon wireless\"\n",
    "    df['category'] = df['category'].apply(lambda x : ' '.join([each for each in x.split(' ') if each != '']))\n",
    "    df['category'] = df['category'].str.lower()  # Turn a whole column into lower \n",
    "\n",
    "    # Helper column will be used when matching later\n",
    "    df['judgement_brand'] = ''\n",
    "    df['match_brand'] = ''\n",
    "    df['match_location_brand'] = ''\n",
    "    df['search_result'] = ''\n",
    "    df['search_result_raw'] = ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reformat the dataframe\n",
    "to_new_df(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 2. Function for getting Hiding Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/533.4 (KHTML, like Gecko) Chrome/5.0.375.99 Safari/533.4',\n",
       " 'Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/10.0.649.0 Safari/534.17',\n",
       " 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/10.0.649.0 Safari/534.17',\n",
       " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.83 Safari/537.1',\n",
       " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.22 (KHTML, like Gecko) Chrome/25.0.1364.152 Safari/537.22',\n",
       " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31',\n",
       " 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36 HubSpot Webcrawler - web-crawlers@hubspot.com',\n",
       " 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36',\n",
       " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36',\n",
       " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.57 Safari/537.36']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrome_ua = pd.read_excel(r'C:\\Anti Blocking.xlsx','chrome_ua')\n",
    "user_agent_list = list(chrome_ua[chrome_ua['os'] == 'Windows']['user agent'])  #All Windows OS on Chrome_ua\n",
    "user_agent_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for random sleep ###\n",
    "\n",
    "def random_sleep(assign=False):\n",
    "    ticket = random.choice(list(range(10)))  # lucky number determine sleep or not\n",
    "    \n",
    "    if assign == False:\n",
    "        i = random.choice(list(range(1,3)))   # Random sleep time\n",
    "    else:\n",
    "        i = assign\n",
    "\n",
    "    if ticket in (0,8): #The number that will triger\n",
    "        print(\"\\nSleep for {} seconds\\n\".format(i))\n",
    "        time.sleep(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 3. Function for crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (Layer 4) Simulate information for call Bing API ###\n",
    "\n",
    "def get_sim_info():\n",
    "    \n",
    "    ### Simulate cv number to pass into API ###\n",
    "    letter_n_number = ascii_uppercase + digits + digits\n",
    "    sim_cvid = ''.join(choice(letter_n_number) for i in range(32))\n",
    "    \n",
    "    ### Simulate form number ###\n",
    "    form_list = ['QBRE','QBLHCN']\n",
    "    sim_form = choice(form_list)\n",
    "    \n",
    "    return(sim_cvid,sim_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 4) Calculate words location based on word's index in all organics ###\n",
    "\n",
    "def smallest_idx(judge_lst, organic_list):\n",
    "\n",
    "    min_match_score = 100000000\n",
    "    min_organic = ''\n",
    "    min_loc = ''\n",
    "    min_word = ''\n",
    "\n",
    "    for brand in judge_lst:\n",
    "        for i in range(len(organic_list)):\n",
    "\n",
    "            if (min_organic == '') or (i<=min_organic):\n",
    "\n",
    "                organic = organic_list[i]\n",
    "                organic_long = len(organic)-2   # The number of text in one SERP organic result, deduct added 2 (blank) in front and back\n",
    "\n",
    "                try:\n",
    "                    match_index = organic.index(brand)+1-1  # Index of matched location. Just wanna lay this out, python index start from 0, so \"+1\". The sentence start with space, so \"-1\"\n",
    "                    match_loc = round(match_index / organic_long,3)  # Text % of place brand named get match in the organic \n",
    "                    match_score = i + 1 + match_loc  # match_score = the_order_of_organic_been_match + location_matched_on_that_organic. Smaller the better\n",
    "\n",
    "                    if match_score < min_match_score:\n",
    "                        min_match_score = match_score\n",
    "                        min_organic = i+1\n",
    "                        min_loc = match_loc\n",
    "                        min_word = brand\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    location = 'organic:{} | location:{}'.format(min_organic,min_loc)\n",
    "    \n",
    "    return(min_organic, location, min_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 3) for getting source from XML for a query ###\n",
    "\n",
    "def get_source_for_query(query):\n",
    "    \n",
    "    ### Get simulated url ###\n",
    "    sim_cvid, sim_form = get_sim_info()\n",
    "    url = 'https://www.bing.com/search?format=rss&q={qr}&qs=n&form={f}&cvid={cid}'.format(qr=query,f=sim_form,cid=sim_cvid)\n",
    "    \n",
    "    ### Get a new agent for crawling ###\n",
    "    agent = random.choice(user_agent_list) \n",
    "    headers = {'User-Agent': agent}\n",
    "    \n",
    "    ### Get the XML source of certain query ###\n",
    "    response = requests.get(url, headers=headers, timeout = 100) \n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    ### Get source for all organics ###\n",
    "    all_source = soup.findAll(\"item\")\n",
    "    \n",
    "    return(all_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 3) for Combining all the title strings, URL and description string from the XML file ###\n",
    "\n",
    "def source_to_str(title_source):\n",
    "    \n",
    "    all_organic_raw = []  # 1. Get a list of original organic title + url from source\n",
    "    title_len = len(title_source)\n",
    "    \n",
    "    for i in range(title_len): \n",
    "        \n",
    "        ### Initialize ###\n",
    "        each = title_source[i]\n",
    "        organic_raw = ''\n",
    "        \n",
    "        ###### Get title for organic ######\n",
    "        \n",
    "        # a. Find title\n",
    "        title_raw = each.find(\"title\").text.replace('<title>','').replace('</title>','')\n",
    "        \n",
    "        # b. Add it to organic_raw string\n",
    "        if title_raw != '':\n",
    "            organic_raw = ' ' + title_raw.lower() + ' '\n",
    "\n",
    "        ###### Get url for organic ######\n",
    "        url_raw = ''\n",
    "        \n",
    "        # a. Try <link>...</link>\n",
    "        url_raw = each.find(\"link\").text.replace('<link>','').replace('</link>','')\n",
    "        \n",
    "        # b. If <link>...</link> couldn't be found, try with Regex \n",
    "        if url_raw == '':\n",
    "            url_raw = re.search(\"<link/>.*<description>\",str(each)).group().replace('<link/>','').replace('<description>','')\n",
    "        \n",
    "        # b. Add it to organic_raw string\n",
    "        if url_raw != '':\n",
    "            organic_raw = organic_raw + url_raw.lower() + ' '\n",
    "            \n",
    "        ###### Grt description for organic ######\n",
    "        \n",
    "        # a. Find description\n",
    "        desc_raw = each.find(\"description\").text.replace('<description>','').replace('</description>','')\n",
    "        \n",
    "        # b. Add it to organic_raw string\n",
    "        if desc_raw != '':\n",
    "            organic_raw = organic_raw + desc_raw.lower() + ' '\n",
    "        \n",
    "        \n",
    "        ### Appending each organic information to a list ###\n",
    "        all_organic_raw.append(organic_raw)\n",
    "    \n",
    "    ### Creating 2 another version of organic result ###\n",
    "    all_organic = [' ' + source_cleaner(x) + ' ' for x in all_organic_raw]  # 2. cleaned version of a list of all organics\n",
    "    all_str = ' '.join(all_organic)  # 3. combine all orgnaics together to 1 string *** Note: Adding blank between organics, make it 3 block between organics, avoid miss match due to front and end connected \n",
    "\n",
    "    \n",
    "    ### Summary ###\n",
    "    # all_str = a string contains all organics information. Will used for matching\n",
    "    # all_organic = a list of organics. Each one in the list is string that has both title + url. The string is cleaned\n",
    "    # all_organic_raw = a list of organics, but not cleaned\n",
    "    \n",
    "    return(all_str, all_organic, all_organic_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 3) for matching brand name in the organic string ###\n",
    "\n",
    "def brand_name_match(domain_brand_list, source_str, organic_list):\n",
    "    \n",
    "    # compare search result to original brand name\n",
    "    judge_lst = [ele for ele in domain_brand_list if (ele in source_str)] # if the original brand name in crawled titles\n",
    "    judge = judge_lst != []\n",
    "    \n",
    "    # Getting the smallest index' percentage to the whole string among the match keywords from brand_list\n",
    "    match_location = ''\n",
    "    match_word = ''\n",
    "    \n",
    "    # Judge true means we could find brand in the SERP page, and the domain have relevancy with the original brand-term\n",
    "    if judge is True:\n",
    "        match_organic_order, match_location, match_word = smallest_idx(judge_lst, organic_list)\n",
    "        \n",
    "    # organic_requirement means the brand have to be in the first 5 organic result\n",
    "    organic_requirement = 5\n",
    "    record_number = 1 if judge is True and match_organic_order <= organic_requirement else 0\n",
    "    \n",
    "    return(record_number, match_word, match_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 2) for getting crawled information, calculate matched location, and load into dataframe ###\n",
    "# After make sure its a website page with result #\n",
    "\n",
    "def crawl_calc_load(df, index):\n",
    "    \n",
    "    ### Initialize ###\n",
    "    row = df.iloc[index]\n",
    "    domain_name = row['clean_domain']\n",
    "    brand_name = row['brand_low']\n",
    "    domain_brand_list = row['brand_list']\n",
    "    similar_domain_idx = list(df[(df['clean_domain'] == domain_name) & (df['brand_low'] == brand_name)].index)\n",
    "    \n",
    "    ### Getting the crawled result if available, try N trails to avoid data lost due to IP-rotating ###\n",
    "    \n",
    "    ## ******************** Note: Why doing this N trails ******************** ##\n",
    "    ## Seems that Threading  will case request failure some time               ##\n",
    "    ## Need to try N times of request to confirm if organics really not exist  ##\n",
    "    ## *********************************************************************** ##\n",
    "    \n",
    "    source_str = ''\n",
    "    count = 0\n",
    "    trails = 10\n",
    "    while source_str == '' and count<trails:\n",
    "        title_source = get_source_for_query(domain_name)   # Getting all organic blocks\n",
    "        source_str, organic_list, organic_raw_list = source_to_str(title_source)  # 1.Source_str contains all title and url string on that page 2.Organic_list contains all title+url string by order\n",
    "        count += 1\n",
    "        \n",
    "        if source_str == \"\":\n",
    "            print(\"SOUP-PROBLEM :: Tried {} {}/{} times\".format(domain_name,count,trails))\n",
    "        \n",
    "    ### Match the brand name to the orgnaic string ###\n",
    "    record_number, match_word, match_location = brand_name_match(domain_brand_list, source_str, organic_list)\n",
    "    \n",
    "    ### if no organic exists; record 'no orgnaic' to dataframe and pass parameter signal as 'nothing' ###\n",
    "    signal = 'nothing'\n",
    "    if source_str == '':\n",
    "        signal = 'does not have organic result'\n",
    "        \n",
    "        organic_list = 'no organic result'\n",
    "        organic_raw_list = ''\n",
    "        \n",
    "    \n",
    "    ### Updating the judgement value (1 means relevant, 0 means no relevancy) for domain with same domain_cleaned value ###\n",
    "    for index in similar_domain_idx: \n",
    "        df.at[index,'judgement_brand'] = record_number  \n",
    "        df.at[index,'search_result'] = organic_list\n",
    "        df.at[index,'search_result_raw'] = organic_raw_list\n",
    "        df.at[index,'match_brand'] = match_word\n",
    "        df.at[index,'match_location_brand'] = match_location\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (layer 1) for getting the crawling result ###\n",
    "\n",
    "def get_result(df, index):\n",
    "    \n",
    "    ### Try to send request and connect to Bing API ###\n",
    "    counter = 0 #Giving more chance to connect\n",
    "    count_end_threshold = 10 #When the loop end\n",
    "    \n",
    "    while counter <= count_end_threshold:   #Trying to open an API for 10 times, if all failed, pass\n",
    "        try:\n",
    "            signal = crawl_calc_load(df, index)\n",
    "            return('success',signal)\n",
    "        except (ConnectionError, requests.exceptions.SSLError, \n",
    "                requests.exceptions.ProxyError, ConnectionResetError, AttributeError) as err:\n",
    "            print(\"Handling run-time error: {}\".format(err))\n",
    "            if counter >= count_end_threshold:\n",
    "                    print(\"\\ncan't connect to API, will go out loop and try again\\n\")\n",
    "                    return('fail',signal)\n",
    "            pass\n",
    "        counter += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 4.1 Crawling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying the dataframe, and prepare for crawling ###\n",
    "\n",
    "\n",
    "df_final = copy.deepcopy(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 1.1 Multi-Threading Crawling ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ****** Function (layer 0) for gett results for all domains [Support Multi-Threading Crawling] ****** ###\n",
    "\n",
    "def get_result_for_all(df_final,i):\n",
    "    \n",
    "    df_len = len(df_final)\n",
    "    \n",
    "    judgement_value = df_final.iloc[i]['judgement_brand'] \n",
    "    if judgement_value == '':    # Didn't make any relevancy judgement yet \n",
    "        ### Getting the crawled data and update the renew_count. Renew a driver if we can't get result about a domain\n",
    "        result = 'fail'\n",
    "        signal = 'nothing' #signal that if orgnaics exist on XML file page\n",
    "        while result == 'fail':\n",
    "            result,signal = get_result(df_final, i)  \n",
    "\n",
    "        ### If no organics on a page ###\n",
    "        if signal == 'does not have organic result':    \n",
    "            dm = df_final.iloc[i]['clean_domain']\n",
    "            print('{}/{} NO ORGANIC FOUND FOR {}'.format(i, df_len-1, dm))\n",
    "        elif signal == 'nothing':\n",
    "            print('{}/{} crawled'.format(i, df_len-1))\n",
    "\n",
    "        ### Implement random sleep after did one crawling\n",
    "        random_sleep(1)\n",
    "\n",
    "    else: # If the judgement made already, go to the next domain\n",
    "        print('{}/{} already have result'.format(i, df_len-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len = len(df_final)\n",
    "df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Multi-Threading Crawling ###\n",
    "\n",
    "\n",
    "start = datetime.now()  # record start time \n",
    "\n",
    "threadlist = []\n",
    "try:\n",
    "    for i in range(df_len):\n",
    "        t = Thread(target=get_result_for_all,args=(df_final,i,))\n",
    "        t.start()\n",
    "        threadlist.append(t)\n",
    "\n",
    "    for b in threadlist:\n",
    "        b.join()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "    pass\n",
    "\n",
    "# Print that crawl is finished\n",
    "print(\"\\nCrawling finished\\n\")\n",
    "\n",
    "end = datetime.now() # record end time \n",
    "print(\"\\n\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Double Check ###\n",
    "\n",
    "df_final[df_final['search_result_raw'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Make a copy of df_final\n",
    "df_final_out = copy.deepcopy(df_final)\n",
    "df_final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [M] 1. Brand URL Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 1. Creating empty Brand+GEO ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (1nd step) for get empty dataframe to store organics for brand+geos ###\n",
    "# max_match means also search (pure geo) like github.fr\n",
    "\n",
    "def get_official_pre(brands,max_match=0):\n",
    "        \n",
    "    ### Get the list of countries ###\n",
    "    geo_file_loc = r'C:\\A_Workplace\\Projects\\Domain Cleaning\\Useful\\geo_file.xlsx'\n",
    "    geo_list = list(pd.read_excel(geo_file_loc,'geo')['geo_list'])\n",
    "    \n",
    "    ### Get the Pure Short Geo list ###\n",
    "    if max_match == 1:\n",
    "        short_geo_list = [\"{} (pure)\".format(x) for x in geo_list if len(x)==2]\n",
    "        geo_list = geo_list + short_geo_list\n",
    "\n",
    "    # Get the number of brands and Geos\n",
    "    numOfBrands = len(brands)\n",
    "    numOfGeo = len(geo_list)\n",
    "    # Geo and brand lists will be used to build official_df_pre dataframe\n",
    "    col_geos = geo_list * numOfBrands\n",
    "    col_brands = sorted(brands * numOfGeo)\n",
    "\n",
    "    # Build the official_df_pre dataframe\n",
    "    official_df_pre = pd.DataFrame({'brand':col_brands, 'geo':col_geos,'organics':''})\n",
    "    return(official_df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 \"brand+geo\" need to get\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###### get EMPTY official_df_pre !!! (Be careful if official_df_pre has result) ######\n",
    "\n",
    "official_df_pre = get_official_pre(brands,0)\n",
    "print('{} \"brand+geo\" need to get'.format(len(official_df_pre)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 2.1 Multi-Threading Crawling ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Function (1st step + layer 2) Getting official URL for a brand ######\n",
    "# cat: assign category to a brand for getting its official site\n",
    "# select_organic_num: treat how many TOP organics as the brand's official information\n",
    "\n",
    "def get_official(brand, geo, cat=0, select_organic_num=3):\n",
    "    \n",
    "    ### Pre-Process ###\n",
    "    brand_list = brand.split(',')\n",
    "    \n",
    "    ### If no value assigned to the parameter \"cat\", get the category value from dataframe ###\n",
    "    if cat == 0:\n",
    "        cat = df_final[df_final['brand_low'] == brand]['category'].unique()[0]  # Category of the brand\n",
    "    \n",
    "    off_organics = []\n",
    "\n",
    "    for each in brand_list:   # For each brand form \"verizon\" and \"verizonwireless\"\n",
    "        \n",
    "        ### For Geo's full name and short name, we use different searching strategy ###\n",
    "        geo_length = len(geo)\n",
    "        if geo_length == 2:\n",
    "            brand_search = \"{}.{} {}\".format(each, geo, cat)  # short country_name + category\n",
    "        elif '(pure)' in geo:\n",
    "            brand_search = \"{}.{}\".format(each, geo.replace('(pure)',''))  # short country_name only\n",
    "        else:\n",
    "            brand_search = \"{}.com {} {}\".format(each, geo, cat) # long country_name + category\n",
    "        \n",
    "        ### Try N Times to get brand+geo related organics ###\n",
    "        brand_search_source = []\n",
    "        count = 0\n",
    "        trails = 10\n",
    "        while brand_search_source == [] and count<trails:\n",
    "            brand_search_source = get_source_for_query(brand_search)  # Get the XML source\n",
    "            count += 1\n",
    "            if brand_search_source == []:\n",
    "                print(\"SOUP-PROBLEM :: Tried {} {}/{} times\".format(each+\"+\"+geo,count,trails))\n",
    "                     \n",
    "        ### Get the organic results for the brand+geo ###\n",
    "        source_str, organic_list, organic_raw_list = source_to_str(brand_search_source)  # Get all organic info\n",
    "        \n",
    "        ### Append the result ###\n",
    "        for each in organic_raw_list[0:select_organic_num]:\n",
    "            off_organics.append(each)\n",
    "    \n",
    "    ### Dedup the off_organics list ###\n",
    "    off_organics  = list(set(off_organics))\n",
    "    \n",
    "    ### Flag if there is no organic result ###\n",
    "    flag = 'does not have organic result' if off_organics == [] else 'nothing'\n",
    "    \n",
    "    return(off_organics,flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (2st step + layer 1), get offical organics for all brands [Support Multi-Threading Crawling]###\n",
    "\n",
    "def get_all_official (official_df_pre,i,organic_num=1):\n",
    "    \n",
    "    num = len(official_df_pre)\n",
    "\n",
    "    ### Judgement value help us see if we have the data we want already ###\n",
    "    off_data_judge = official_df_pre.iloc[i]['organics'] \n",
    "    brand = official_df_pre.iloc[i]['brand']\n",
    "    geo = official_df_pre.iloc[i]['geo']\n",
    "\n",
    "    ### Crawl only when the organics is empty ###\n",
    "    if off_data_judge == '':\n",
    "        \n",
    "        ### Try to crawl until get it, except meet special error ###\n",
    "        continue_judge = 1\n",
    "        while continue_judge == 1:\n",
    "            try:\n",
    "                off_organics,theFlag = get_official(brand, geo, cat=0, select_organic_num=organic_num)  # cat=0 means the function will find category form dataframe\n",
    "                official_df_pre.at[i,'organics'] = off_organics\n",
    "                \n",
    "                ### Flag it if there is not result after several try ###\n",
    "                if theFlag == 'nothing':\n",
    "                    print('{}/{} {} {}'.format(i+1,num,brand,geo))  \n",
    "                elif theFlag == 'does not have organic result':\n",
    "                    print('{}/{} NO ORGANIC FOUND FOR {} {}'.format(i+1,num,brand,geo))\n",
    "                \n",
    "                random_sleep(1) # After crawl each brand, random sleep a while, assign 3 seconds sleep\n",
    "                continue_judge = 0\n",
    "            except (ConnectionError, requests.exceptions.SSLError, \n",
    "                    requests.exceptions.ProxyError, ConnectionResetError, AttributeError) as err:\n",
    "                print(\"Handling run-time error: {}\".format(err))\n",
    "                pass\n",
    "    else:\n",
    "            print('{}/{} {} {} already have result'.format(i+1,num,brand,geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Multi-Threading Crawling ###\n",
    "\n",
    "start = datetime.now()  # record start time \n",
    "\n",
    "df_long = len(official_df_pre)\n",
    "threadlist = []\n",
    "\n",
    "try:\n",
    "    for i in range(df_long ):\n",
    "        t = Thread(target=get_all_official,args=(official_df_pre, i, 2,))\n",
    "        t.start()\n",
    "        threadlist.append(t)\n",
    "\n",
    "    for b in threadlist:\n",
    "        b.join()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "    pass\n",
    "\n",
    "print(\"\\nCrawling finished\\n\")\n",
    "\n",
    "\n",
    "end = datetime.now() # record end time \n",
    "print(\"\\n\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_df_pre_out = copy.deepcopy(official_df_pre)\n",
    "official_df_pre_out.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 3. Organize and Clean crawled organics ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (3rd step) to organize official_df_pre to official_df final version ###\n",
    "\n",
    "def pre_to_final(df_pre):\n",
    "    \n",
    "    #Creating an organized dataframe\n",
    "    df = pd.DataFrame({'brand':brands, 'organics':''}) \n",
    "    \n",
    "    # Load from df_pre to df\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        brand = df['brand'].iloc[i]\n",
    "    \n",
    "        # Get all organics from df_pre\n",
    "        all_organics = list(df_pre[df_pre['brand'] == brand]['organics'])\n",
    "    \n",
    "        # Get unique organics\n",
    "        all_organics = list(set([x for each in all_organics for x in each]))\n",
    "        \n",
    "        # Load\n",
    "        df.iloc[i]['organics'] = all_organics\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (4th step + layer 2) to extract URL link from a line of word\n",
    "# eg. 'http://formation.rue89.com/' ---> 'formation rue89 com'\n",
    "\n",
    "\n",
    "def extract_url (url_text):\n",
    "    \n",
    "    ### pattern for finding url \n",
    "    re_pattern = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url_raw = [x[0] for x in re.findall(re_pattern, url_text)]\n",
    "    \n",
    "    core_list = []\n",
    "    ### Clean the extra information like \"http\" cleaned website ###\n",
    "    for each in url_raw:\n",
    "        if ('https://' in each) or ('http://' in each):\n",
    "            url_split = each.split('/')\n",
    "            try:\n",
    "                core = [x for x in url_split if '.com' in x][0]  # If splitted part has \".com\" get this term\n",
    "            except:\n",
    "                core = each.split('/')[2] # Core part in a url, for example \"https://www.riide.com/tech-specs\" ---> \"www.riide.com\"\n",
    "        elif ('https://' not in each) and ('http://' not in each) and ('www' in each):\n",
    "            url_split = each.split('/')\n",
    "            try:\n",
    "                core = [x for x in url_split if 'www.' in x][0] # If splitted part has \"www.\" get this term\n",
    "            except:\n",
    "                core = each.split('/')[0]\n",
    "        else:\n",
    "            core = \"\"\n",
    "            \n",
    "        core_clean = source_cleaner(core)\n",
    "        core_remove = re.sub('https|http|www|','',core_clean)\n",
    "        core_final = ' ' + ' '.join([each for each in core_remove.split(' ') if each != '']) + ' '\n",
    "        \n",
    "        if core_final != '  ':\n",
    "            core_list.append(core_final)\n",
    "    \n",
    "    return(core_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Function (4th step + layer 1) Extract core official url information from organic ######\n",
    "\n",
    "def clean_official(official):\n",
    "    result = []\n",
    "    for each in official:\n",
    "        \n",
    "        temp = extract_url (each)\n",
    "        result.append(temp[0])\n",
    "    \n",
    "    result = list(sorted(set(result)))  # De-dup\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (5th step) Reorder the clean_url list, make the url start with brand's char listed in the front ###\n",
    "\n",
    "def reorder_url(row):\n",
    "    brand_first_letter = row['brand'][0]\n",
    "    clean_url = row['clean_url']\n",
    "    \n",
    "    ### The url to put in the front and back ###\n",
    "    front_element = [x for x in clean_url if x.startswith(\" {}\".format(brand_first_letter))]\n",
    "    back_element = [y for y in clean_url if y not in front_element]\n",
    "    \n",
    "    ### Combine all the url and return ###\n",
    "    out_url = front_element + back_element\n",
    "    \n",
    "    row['clean_url'] = out_url\n",
    "    \n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get the organized final version official df ###\n",
    "\n",
    "official_df = pre_to_final(official_df_pre)\n",
    "official_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Getting official url's core information\n",
    "\n",
    "official_df['clean_url'] = official_df['organics'].apply(clean_official)\n",
    "official_df = official_df.apply(reorder_url,axis=1)\n",
    "official_df['pick_url'] = official_df['clean_url']\n",
    "official_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Loading the official_df to a outside Excel for cleaning and organizing ######\n",
    "\n",
    "print(\"Adding Official URL to the Taxonomy: {}\\n\".format(taxonomy_loc))\n",
    "\n",
    "### The Taxonomy File and the Sheet to Edit ###\n",
    "workbook=load_workbook(taxonomy_loc)\n",
    "psn = 'official url prep'  #psn = prep sheet name\n",
    "\n",
    "### Try to delete the \"official url prep\" sheet if any\n",
    "try:\n",
    "    del workbook[psn]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "### Adding organized url sheet \"official\" to the Excel ###\n",
    "osn = 'official'  # osn = Organized sheet name\n",
    "try:\n",
    "    workbook[osn]  # Check if 'official' sheet exist, if not create a new sheet\n",
    "except KeyError:\n",
    "    workbook.create_sheet(osn)\n",
    "    \n",
    "### Save the workbook edit above ###    \n",
    "workbook.save(taxonomy_loc)\n",
    "\n",
    "### Loading official url into excel ###\n",
    "writer = pd.ExcelWriter(taxonomy_loc, engine='openpyxl') \n",
    "book = load_workbook(taxonomy_loc) # Check other sheet in the excel; prevent over-writing and data loss\n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "official_df.to_excel(writer, sheet_name = psn, index=0) # Load the official dataframe to the excel sheet \n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "print('\\nOfficial_df loaded into the Excel sheet successfully!\\n')\n",
    "print('\\n*** Please move organized official_df from the TAB [Official URL Prep] to [Official] in the Taxonomy *** \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 4. Organize or Modify the official_df dataframe ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for converting string item in Excel to a list of string ###\n",
    "\n",
    "def restore_list_from_excel(string):\n",
    "    trim_string = ' '.join([x for x in string.split(' ') if x != ''])  # Trim the extra space in the string \n",
    "    new_string = trim_string[1:][0:-1]  # Remove \"[\" and \"]\"\n",
    "    \n",
    "    splited_list = new_string.split(',')  # split the url according to the comma between them \",\"\n",
    "    \n",
    "    ### for each url, reomve \"'\"; then remove extra space; lastly, adding back the front and back space ###\n",
    "    new_splited_list = [' ' + ' '.join([a for a in x.replace(\"'\",\"\").split(' ') if a!='']) + ' ' for x in splited_list]\n",
    "    return(new_splited_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for converting the organized final_df to a URL dataframe ###\n",
    "\n",
    "def excel_df_to_url_df(organized_df):\n",
    "    \n",
    "    organized_df['organics'] = organized_df['organics'].apply(restore_list_from_excel)\n",
    "    organized_df['clean_url'] = organized_df['clean_url'].apply(restore_list_from_excel)\n",
    "    organized_df['pick_url'] = organized_df['pick_url'].apply(restore_list_from_excel)\n",
    "    \n",
    "    return(organized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the organized official url from the Taxonomy file - official sheet\n",
    "print(\"Will read TAB [Official] from Taxonomy file: {}\".format(taxonomy_loc))\n",
    "organized_df = pd.read_excel(taxonomy_loc,'official')\n",
    "\n",
    "### From the Excel df to organized url df ###\n",
    "org_official_df = excel_df_to_url_df(organized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_official_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :: 4. Work on Final DataFrame ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (5th step) Getting URL from organic ###\n",
    "\n",
    "def get_organic_url (organic_raw_list):\n",
    "    \n",
    "    organic_url = []\n",
    "    for org in organic_raw_list:\n",
    "        url_list = extract_url(org)\n",
    "        url_list = list(set(url_list))\n",
    "        organic_url.append(url_list)\n",
    "        \n",
    "    return(organic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function (Last step) Matching brand_url to oganic url (Exact Match) ###\n",
    "\n",
    "def url_match(row):\n",
    "    organic_url = row['organic_url']  # URL from organic result\n",
    "    url_list = row['url_list']  # brand's official URL\n",
    "    \n",
    "    ### Only match to organic url from the first 3 organics \n",
    "    num = len(organic_url)\n",
    "    threshold = 3 \n",
    "    \n",
    "    ### Load all organic_url to a list and record which organic are they from\n",
    "    o_url = []\n",
    "    o_url_from = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        one_list = organic_url[i]      \n",
    "        for each in one_list:\n",
    "            if each not in o_url:  # Only unique URL \n",
    "                o_url.append(each)\n",
    "                o_url_from.append(i)\n",
    "    \n",
    "    ### Matching brand_url to organic_url\n",
    "    match_url = []\n",
    "    match_organic = []\n",
    "    \n",
    "    for each in url_list:\n",
    "        if each in o_url:\n",
    "            idx = o_url.index(each)  # Get index of url match in combined organic url list\n",
    "            which_organic = o_url_from[idx]  # Get the orgnaic order of that match url, where it comes from\n",
    "            \n",
    "            match_url.append(each)\n",
    "            match_organic.append(which_organic+1)\n",
    "    \n",
    "    ### Judgement and prepare to load ###\n",
    "    if match_url != []:\n",
    "        \n",
    "        if any(x <= threshold for x in match_organic):\n",
    "            judge = 1\n",
    "        else:\n",
    "            judge = 0\n",
    "            \n",
    "        match_url = list(set(match_url))\n",
    "        \n",
    "        match_organic = list(set(match_organic))\n",
    "        match_organic = ['{}'.format(x) for x in match_organic]\n",
    "        sentence = \"organic: \" + \",\".join(match_organic)\n",
    "    else:\n",
    "        judge = 0\n",
    "        match_url = \"\"\n",
    "        sentence = \"\"\n",
    "    \n",
    "    row['judgement_url'] = judge\n",
    "    row['match_url'] = match_url\n",
    "    row['match_location_url'] = sentence\n",
    "    \n",
    "    return(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting URLs from organic results ###\n",
    "df_final['organic_url'] = df_final['search_result_raw'].apply(get_organic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding official_url to final df###\n",
    "df_final['url_list'] = df_final['brand_low'].apply(lambda b : org_official_df[org_official_df['brand'] == b].iloc[0]['pick_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### URL Matching ###\n",
    "df_final = df_final.apply(url_match, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [M] 2.Domain Keyword Negative Word Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###### Match negative word in domain ######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain_picked = list(pd.read_excel('negative_word.xlsx',sheet_name='domain_picked')['keyword'])  # Hand picked domain filter keywords\n",
    "compliance = list(pd.read_excel('negative_word.xlsx',sheet_name='compliance')['keyword'])  # Compliance given domain filter keywords\n",
    "\n",
    "domain_safety = [str(x) for x in list(set(domain_picked + compliance))]  # Combine\n",
    "domain_safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Function for finding irrelevant words like \"career\", \"enterprise\" in domain ######\n",
    "\n",
    "def find_irrelevant(row):\n",
    "    domain = row['domain']\n",
    "    irrelevant_list = [x for x in domain_safety if x in domain]\n",
    "    \n",
    "    if irrelevant_list != []:\n",
    "        judge = 1\n",
    "        out = irrelevant_list\n",
    "    elif irrelevant_list == []:\n",
    "        judge = 0\n",
    "        out = \"no irrelevant in domain\"\n",
    "        \n",
    "    row['domain_has_negative'] = judge\n",
    "    row['domain_negative'] = out\n",
    "    \n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.apply(find_irrelevant, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [M] 3.Organic illegal Word Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_picked = list(pd.read_excel('negative_word.xlsx',sheet_name='organic_picked')['keyword'])  # Hand picked domain filter keywords\n",
    "compliance = list(pd.read_excel('negative_word.xlsx',sheet_name='compliance')['keyword'])  # Compliance given domain filter keywords\n",
    "\n",
    "organic_safety = [' '+str(x)+' ' for x in list(set(organic_picked + compliance))]  # Combine\n",
    "organic_safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_illegal(row):\n",
    "    \n",
    "    organic_list = row['search_result']\n",
    "    \n",
    "    long = len(organic_list)\n",
    "    threshold = 3      # Check if sensitive word appear in the first 3 organics\n",
    "    \n",
    "    if long > threshold:\n",
    "        long = threshold\n",
    "    \n",
    "    # Check if there is sensitive words in first 3 organics\n",
    "    sensitive_list = []\n",
    "    for i in range(long):\n",
    "        organic = organic_list[i]\n",
    "        check = [w for w in organic_safety if w in organic]\n",
    "        if check != []:\n",
    "            check = [x.replace(' ','') for x in check]\n",
    "            out = \"organic{} \".format(i+1) + \"[\" + \",\".join(check) + \"]\"\n",
    "            sensitive_list.append(out)\n",
    "    \n",
    "    # Load result to excel\n",
    "    if sensitive_list != []:\n",
    "        judge = 1\n",
    "    else:\n",
    "        judge = 0\n",
    "        sensitive_list = \"no illegal\"\n",
    "    \n",
    "    row['organic_has_negative'] = judge\n",
    "    row['organic_negative'] = sensitive_list\n",
    "    \n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.apply(find_illegal, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [M] 4. Non-English Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_eng_detector(string):\n",
    "    \n",
    "    ### RE pattern: Don't want latin-char, number, and non-char symbols\n",
    "    extra = '“”‘’!\"#$%&\\'()*+,-./:；;<=>?@[\\\\]【】^_——`{|}~\\n。！，…ːʻˈ–®•→›🙏ᐅᐉߴ\\s'     #Supplement list\n",
    "    RE = re.compile('[^a-zA-Z0-9\\W+{}]+'.format(extra))\n",
    "    other = re.findall(RE, string)\n",
    "    return(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_english(row):\n",
    "    \n",
    "    ### organic list\n",
    "    organic_raw_list = row['search_result_raw']\n",
    "    \n",
    "    long = len(organic_raw_list)\n",
    "    threshold = 3      # Check if sensitive word appear in the first 3 organics\n",
    "    \n",
    "    if long > threshold:\n",
    "        long = threshold   # Only check first 3 organics \n",
    "    \n",
    "    other_language = []\n",
    "    for i in range(long):\n",
    "        organic_raw = organic_raw_list[i]\n",
    "        \n",
    "        other = non_eng_detector(organic_raw)\n",
    "        \n",
    "        if other != []:\n",
    "            out = \"organic{} \".format(i+1) + \"[\" + \" \".join(other) + \"]\"\n",
    "            other_language.append(out)\n",
    "            \n",
    "    if other_language == []:\n",
    "        other_language = \"no other language\"\n",
    "        judge = 0\n",
    "    else:\n",
    "        judge = 1\n",
    "        \n",
    "    row['organic_has_nonEnglish'] = judge\n",
    "    row['non_english'] = other_language\n",
    "    \n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.apply(find_non_english, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [M] 2.2 Rematch Brand_list After Crawl (If Necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Fuction: Get match for brand_name, and their match location ######\n",
    "\n",
    "def after_crawl_match(row):\n",
    "    \n",
    "    # 1. Basic search result information needed for both matching url & brand\n",
    "    target = row['brand_list']\n",
    "    search_result = row['search_result']\n",
    "    result_sentence = ' '.join(row['search_result'])  # Added an extra blank between organic results (3 total) to differentiate them and avoid mismtch\n",
    "    \n",
    "    record_number, match_word, match_location = brand_name_match(target, result_sentence, search_result)\n",
    "    \n",
    "    return(record_number, match_word, match_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Loading brand match information into dataframe (Rematch after crawl) ### \n",
    "\n",
    "def brand_match(row):\n",
    "   \n",
    "    # Get url match result\n",
    "    record_number, brand_match, brand_match_location = after_crawl_match(row)\n",
    "    \n",
    "    # Load into df\n",
    "    row['judgement_brand'] = record_number  \n",
    "    row['match_brand'] = brand_match\n",
    "    row['match_location_brand'] = brand_match_location\n",
    "\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get brand_low and brand_list again\n",
    "df_final['brand_low'] = df_final['brand'].str.lower() \n",
    "df_final['brand_list'] = df_final['brand_low'].apply(to_brand_list)\n",
    "\n",
    "# Erase original match\n",
    "df_final['judgement_brand'] = ''\n",
    "df_final['match_brand'] = ''\n",
    "df_final['match_location_brand'] = ''\n",
    "\n",
    "### Find brand match judgement, matched_url and their match_location ###\n",
    "df_final = df_final.apply(brand_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_out = copy.deepcopy(df_final)\n",
    "df_final_out.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X1. Reorganize the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put the columns in the dataframe into the order as the following ###\n",
    "\n",
    "column_order = ['brand',\n",
    "             'domain',\n",
    "             'cluster_id',\n",
    "             'category',\n",
    "             'clean_domain',\n",
    "             'brand_low',\n",
    "             'judgement_brand',\n",
    "             'judgement_url',\n",
    "             'domain_has_negative',\n",
    "             'organic_has_negative',\n",
    "             'organic_has_nonEnglish',\n",
    "             'brand_list',\n",
    "             'match_brand',\n",
    "             'match_location_brand',\n",
    "             'domain_negative',\n",
    "             'organic_negative',\n",
    "             'non_english',\n",
    "             'url_list',\n",
    "             'match_url',\n",
    "             'match_location_url',\n",
    "             'organic_url',\n",
    "             'search_result',\n",
    "             'search_result_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dictionary to substitute the column name if its a dataframe for the generic domains ###\n",
    "\n",
    "substitute_dict = {'brand':'product',\n",
    "                   'brand_low':'product_low',\n",
    "                   'judgement_brand':'judgement_product',\n",
    "                   'brand_list':'product_list',\n",
    "                   'match_brand':'match_product',\n",
    "                   'match_location_brand':'match_location_product'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taxonomy name: {}\".format(taxonomy_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the organized dataframe ###\n",
    "\n",
    "if 'generic' in taxonomy_name:\n",
    "    new_column_order = [x for x in column_order if ('url' not in x) and (x != 'category')] # Get columns name for generic domains\n",
    "    df_final_reorg = df_final[new_column_order] # Reorder the dataframe \n",
    "    df_final_reorg = df_final_reorg.rename(columns = substitute_dict) # Change name for columns\n",
    "else:  # If not indicate the Taxonomy is a Generic list\n",
    "    try: # Using all the columns\n",
    "        df_final_reorg = df_final[column_order]\n",
    "    except KeyError:  # If do not have URL related columns, ignore that\n",
    "        new_column_order = [x for x in column_order if 'url' not in x]\n",
    "        df_final_reorg = df_final[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_reorg.to_clipboard(index=0)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
